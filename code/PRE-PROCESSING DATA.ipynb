{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction and Pre-processing from NADRA Website\n",
        "In this notebook, i extract text data from multiple pages on the NADRA website. I use BeautifulSoup library to extract data and Pre-process each html page using REGEX. The raw HTML content is saved into individual HTML files, and the processed text is stored in separate text files.\n"
      ],
      "metadata": {
        "id": "zYCczNFEAsUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Import Libraries and Set Up URLs"
      ],
      "metadata": {
        "id": "HHpsu4VbAiLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "# 10 html pages from the nadra.gov.pk website\n",
        "urls = [\n",
        "    \"https://www.nadra.gov.pk/\",\n",
        "    \"https://www.nadra.gov.pk/international-projects/\",\n",
        "    \"https://www.nadra.gov.pk/local-projects/\",\n",
        "    \"https://www.nadra.gov.pk/identity/\",\n",
        "    \"https://www.nadra.gov.pk/national-identity-card/\",\n",
        "    \"https://www.nadra.gov.pk/downloads/\",\n",
        "    \"https://tenders.nadra.gov.pk/\",\n",
        "    \"https://www.nadra.gov.pk/media-releases/\",\n",
        "    \"https://www.nadra.gov.pk/operational-management/\",\n",
        "    \"https://careers.nadra.gov.pk/\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "l8dqneQDAoFZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract HTML Content from URLs and store HTML pages"
      ],
      "metadata": {
        "id": "MuqeSpnXA3-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# method for extracting html content from url\n",
        "def get_html(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    return None\n",
        "\n",
        "# Extract and store html pages\n",
        "html_pages = []   # list for storing html content\n",
        "for url in urls:\n",
        "    page_html = get_html(url)\n",
        "    if page_html:\n",
        "        html_pages.append(page_html)\n",
        "        #  save the html to files\n",
        "        with open(f\"html_file_{urls.index(url)+1}.html\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(page_html)\n",
        "\n",
        "print(f\"{len(html_pages)} html pages extracted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NANRYZxBFsC",
        "outputId": "065aa849-2fd4-4e7b-89e4-9284a2a5cafa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 html pages extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean HTML Pages Using BeautifulSoup and Regex.\n",
        "And Store cleaned data."
      ],
      "metadata": {
        "id": "jfwlRsqjBMd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pwA0QE1a-ejh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8418d41-7389-4d00-ed8d-a72675bcf00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "html pages are cleaned and saved.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# method to clean all html tags, special characters, advertisements/ extra link\n",
        "def clean_html(raw_html):\n",
        "    # Remove all HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
        "\n",
        "  # Remove all img tags\n",
        "    for img in soup.find_all('img'):\n",
        "        img.decompose()\n",
        "\n",
        "    # Remove ads\n",
        "    ad_classes = ['ad', 'advertisement','advertisements', 'sponsored']\n",
        "    for ad_class in ad_classes:\n",
        "        for ad in soup.find_all(class_=ad_class):\n",
        "            ad.decompose()\n",
        "\n",
        "    clean_text = soup.get_text()\n",
        "\n",
        "    # Remove special characters using regex\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text)\n",
        "\n",
        "    # remove extra links/urls\n",
        "    clean_text = re.sub(r'http\\S+', '', clean_text)\n",
        "\n",
        "    return clean_text.strip()\n",
        "\n",
        "# Clean the HTML pages\n",
        "cleaned_pages = [] # list for storing cleaned content\n",
        "for html in html_pages:\n",
        "    cleaned_text = clean_html(html)\n",
        "    cleaned_pages.append(cleaned_text)\n",
        "\n",
        "# Save cleaned text files\n",
        "for i, text in enumerate(cleaned_pages):\n",
        "    with open(f\"Cleaned_file_{i+1}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text)\n",
        "\n",
        "print(\"html pages are cleaned and saved.\")"
      ]
    }
  ]
}